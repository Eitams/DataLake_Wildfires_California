{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This notebook helped to build the script, it is not formatted and not clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build lambda for RDS datalake for Weather data (real time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prompt data from S3 bucket, ThermoAnomalies data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 # make --> pip install boto3\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client =boto3.client('s3')\n",
    "s3_bucket_name='eswf1'\n",
    "s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id='ASIAYQYC6XJONKLN73D3',\n",
    "                    aws_secret_access_key='gRIl6dY6V38iB+slTJIi1MFjG7O6i1vjTVL9y802',\n",
    "                    aws_session_token='FwoGZXIvYXdzEPT//////////wEaDFc1mOrInlnZTBLg+CK5AeMZ2B7SSwZKTt1PxgPyV8Bon0hwZYUpZsQa6SIODxW6Ka4DGps94XB/aPb0pznrbDvox4VEU6Eq7FCL/OOk57LYRU7NIv1ZjZ93XWKqtb8KBFPCH1kXTJE/B8Suid3K1iUCHzgzF0SK5Frfd5Z9k3znriuiBQkRuCbTi0T/dVUaRUO1FM0cUpg1Wq0eHWJSevCOD1Q0l7/ORDVUJd79Q0MTlNuuEmtPGm874/0AIzRZRMTsNC3aCmjsKOrsrJIGMi1q5VHRe4cYvaSqQ9am+MIgWMAvAD81OYFJB66cBpyhGlXrCgHAU5ZYZLFRhDs=')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='eswf1', key='ThA_layer.zip')\n",
      "s3.ObjectSummary(bucket_name='eswf1', key='ThA_layer2.zip')\n",
      "s3.ObjectSummary(bucket_name='eswf1', key='ThA_layer3.zip')\n",
      "s3.ObjectSummary(bucket_name='eswf1', key='ThAnomalies.csv')\n",
      "s3.ObjectSummary(bucket_name='eswf1', key='ThAnomalies2.csv')\n",
      "s3.ObjectSummary(bucket_name='eswf1', key='ThAnomalies3.csv')\n",
      "3\n",
      "['ThAnomalies.csv', 'ThAnomalies2.csv', 'ThAnomalies3.csv']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Getting data files from the AWS S3 bucket as denoted above and printing the first 10 file names having prefix \"2019/7/8\" \"\"\"\n",
    "my_bucket=s3.Bucket(s3_bucket_name)\n",
    "\n",
    "bucket_list=[]\n",
    "for my_bucket_object in my_bucket.objects.all():\n",
    "    file_name=my_bucket_object.key\n",
    "    print(my_bucket_object)\n",
    "    if file_name.find(\".csv\")!=-1:\n",
    "        bucket_list.append(my_bucket_object.key)\n",
    "length_bucket_list=print(len(bucket_list))\n",
    "print(bucket_list[0:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>BRIGHTNESS</th>\n",
       "      <th>SCAN</th>\n",
       "      <th>TRACK</th>\n",
       "      <th>SATELLITE</th>\n",
       "      <th>CONFIDENCE</th>\n",
       "      <th>VERSION</th>\n",
       "      <th>BRIGHT_T31</th>\n",
       "      <th>FRP</th>\n",
       "      <th>ACQ_DATE</th>\n",
       "      <th>DAYNIGHT</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2195</td>\n",
       "      <td>330.69</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>T</td>\n",
       "      <td>86</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>307.02</td>\n",
       "      <td>20.33</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-122.05948</td>\n",
       "      <td>39.35455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2204</td>\n",
       "      <td>329.77</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.05</td>\n",
       "      <td>T</td>\n",
       "      <td>83</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>304.23</td>\n",
       "      <td>23.31</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-120.03122</td>\n",
       "      <td>36.27418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2206</td>\n",
       "      <td>325.20</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.05</td>\n",
       "      <td>T</td>\n",
       "      <td>68</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>302.81</td>\n",
       "      <td>18.38</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-120.02637</td>\n",
       "      <td>36.26736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2194</td>\n",
       "      <td>323.71</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>T</td>\n",
       "      <td>81</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>306.35</td>\n",
       "      <td>12.32</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-122.07099</td>\n",
       "      <td>39.35618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2193</td>\n",
       "      <td>321.85</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.06</td>\n",
       "      <td>T</td>\n",
       "      <td>58</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>307.68</td>\n",
       "      <td>8.82</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-118.79675</td>\n",
       "      <td>38.96482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2197</td>\n",
       "      <td>320.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>T</td>\n",
       "      <td>71</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>300.55</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-121.59753</td>\n",
       "      <td>38.90963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2205</td>\n",
       "      <td>319.28</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.05</td>\n",
       "      <td>T</td>\n",
       "      <td>57</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>302.36</td>\n",
       "      <td>10.88</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-120.03841</td>\n",
       "      <td>36.26934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2202</td>\n",
       "      <td>317.44</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.04</td>\n",
       "      <td>T</td>\n",
       "      <td>64</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>295.40</td>\n",
       "      <td>11.29</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-119.98958</td>\n",
       "      <td>36.61306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2192</td>\n",
       "      <td>317.28</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.01</td>\n",
       "      <td>T</td>\n",
       "      <td>73</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>303.63</td>\n",
       "      <td>10.05</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-122.25343</td>\n",
       "      <td>39.54778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2201</td>\n",
       "      <td>317.21</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.04</td>\n",
       "      <td>T</td>\n",
       "      <td>70</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>297.83</td>\n",
       "      <td>11.71</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-120.00159</td>\n",
       "      <td>36.61505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2196</td>\n",
       "      <td>315.93</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.02</td>\n",
       "      <td>T</td>\n",
       "      <td>71</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>298.20</td>\n",
       "      <td>12.14</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-123.10704</td>\n",
       "      <td>39.30494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2198</td>\n",
       "      <td>313.92</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>T</td>\n",
       "      <td>69</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>296.84</td>\n",
       "      <td>8.28</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-122.02831</td>\n",
       "      <td>38.77820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2203</td>\n",
       "      <td>313.59</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.05</td>\n",
       "      <td>T</td>\n",
       "      <td>50</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>299.73</td>\n",
       "      <td>6.78</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-119.94797</td>\n",
       "      <td>36.45555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2190</td>\n",
       "      <td>313.05</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>T</td>\n",
       "      <td>50</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>301.09</td>\n",
       "      <td>6.33</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-122.00409</td>\n",
       "      <td>39.77106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2191</td>\n",
       "      <td>312.51</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>T</td>\n",
       "      <td>54</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>300.46</td>\n",
       "      <td>5.45</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-121.99249</td>\n",
       "      <td>39.76943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2199</td>\n",
       "      <td>312.08</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>T</td>\n",
       "      <td>65</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>298.69</td>\n",
       "      <td>5.72</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-122.02828</td>\n",
       "      <td>38.70437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2200</td>\n",
       "      <td>308.80</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.01</td>\n",
       "      <td>T</td>\n",
       "      <td>59</td>\n",
       "      <td>6.1NRT</td>\n",
       "      <td>294.40</td>\n",
       "      <td>5.47</td>\n",
       "      <td>1648839180000</td>\n",
       "      <td>D</td>\n",
       "      <td>-120.53757</td>\n",
       "      <td>38.24928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    OBJECTID  BRIGHTNESS  SCAN  TRACK SATELLITE  CONFIDENCE VERSION  \\\n",
       "0       2195      330.69  1.01   1.00         T          86  6.1NRT   \n",
       "1       2204      329.77  1.10   1.05         T          83  6.1NRT   \n",
       "2       2206      325.20  1.10   1.05         T          68  6.1NRT   \n",
       "3       2194      323.71  1.01   1.00         T          81  6.1NRT   \n",
       "4       2193      321.85  1.13   1.06         T          58  6.1NRT   \n",
       "5       2197      320.72  1.00   1.00         T          71  6.1NRT   \n",
       "6       2205      319.28  1.10   1.05         T          57  6.1NRT   \n",
       "7       2202      317.44  1.10   1.04         T          64  6.1NRT   \n",
       "8       2192      317.28  1.01   1.01         T          73  6.1NRT   \n",
       "9       2201      317.21  1.10   1.04         T          70  6.1NRT   \n",
       "10      2196      315.93  1.04   1.02         T          71  6.1NRT   \n",
       "11      2198      313.92  1.00   1.00         T          69  6.1NRT   \n",
       "12      2203      313.59  1.10   1.05         T          50  6.1NRT   \n",
       "13      2190      313.05  1.01   1.00         T          50  6.1NRT   \n",
       "14      2191      312.51  1.01   1.00         T          54  6.1NRT   \n",
       "15      2199      312.08  1.00   1.00         T          65  6.1NRT   \n",
       "16      2200      308.80  1.03   1.01         T          59  6.1NRT   \n",
       "\n",
       "    BRIGHT_T31    FRP       ACQ_DATE DAYNIGHT          x         y  \n",
       "0       307.02  20.33  1648839180000        D -122.05948  39.35455  \n",
       "1       304.23  23.31  1648839180000        D -120.03122  36.27418  \n",
       "2       302.81  18.38  1648839180000        D -120.02637  36.26736  \n",
       "3       306.35  12.32  1648839180000        D -122.07099  39.35618  \n",
       "4       307.68   8.82  1648839180000        D -118.79675  38.96482  \n",
       "5       300.55  12.55  1648839180000        D -121.59753  38.90963  \n",
       "6       302.36  10.88  1648839180000        D -120.03841  36.26934  \n",
       "7       295.40  11.29  1648839180000        D -119.98958  36.61306  \n",
       "8       303.63  10.05  1648839180000        D -122.25343  39.54778  \n",
       "9       297.83  11.71  1648839180000        D -120.00159  36.61505  \n",
       "10      298.20  12.14  1648839180000        D -123.10704  39.30494  \n",
       "11      296.84   8.28  1648839180000        D -122.02831  38.77820  \n",
       "12      299.73   6.78  1648839180000        D -119.94797  36.45555  \n",
       "13      301.09   6.33  1648839180000        D -122.00409  39.77106  \n",
       "14      300.46   5.45  1648839180000        D -121.99249  39.76943  \n",
       "15      298.69   5.72  1648839180000        D -122.02828  38.70437  \n",
       "16      294.40   5.47  1648839180000        D -120.53757  38.24928  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = s3.Object(s3_bucket_name,key='ThAnomalies3.csv')\n",
    "data=obj.get()['Body'].read()\n",
    "df=pd.read_csv(io.BytesIO(data), header=0, delimiter=\",\", low_memory=False)\n",
    "thdf= df.drop('Unnamed: 0', axis=1)\n",
    "thdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data manipulation to access locations + Load csv file in RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    #print(\"host={} dbname={} user={} password={}\".format(ENDPOINT, DB_NAME, USERNAME, PASSWORD))\n",
    "    #conn = psycopg2.connect(\"host={} dbname={} user={} password={}\".format(ENDPOINT, DB_NAME, USERNAME, PASSWORD))\n",
    "    conn = psycopg2.connect(\"host=datalake.cbqjcib7k7og.us-east-1.rds.amazonaws.com dbname=datalake_weather user=nathan password=123456789\")\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not make connection to the Postgres database\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    cur = conn.cursor()\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not get curser to the Database\")\n",
    "    print(e)\n",
    "    \n",
    "# Auto commit is very important\n",
    "conn.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"CREATE TABLE IF NOT EXISTS THERMOANOMALIES_DATA (OBJECTID INT, BRIGHTNESS FLOAT, SCAN FLOAT, TRACK FLOAT, SATELLITE VARCHAR(10), CONFIDENCE FLOAT, VERSION VARCHAR(15), BRIGHT_T31 FLOAT, FRP FLOAT, ACQ_DATE FLOAT, DAYNIGHT VARCHAR(10), x FLOAT, y FLOAT)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of imported rows:  17\n"
     ]
    }
   ],
   "source": [
    "#Upload data in the table\n",
    "data=thdf\n",
    "\n",
    "cols = \",\".join([str(i) for i in data.columns.tolist()])\n",
    "\n",
    "imported_rows=0\n",
    "excluded_rows_index=[]\n",
    "for i,row in data.iterrows():\n",
    "    try:\n",
    "        sql = \"INSERT INTO THERMOANOMALIES_DATA  (\" +cols + \") VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "        cur.execute(sql, tuple(row))\n",
    "        conn.commit()\n",
    "        imported_rows+=1\n",
    "    except:\n",
    "        excluded_rows_index.append(i)\n",
    "        print(i)\n",
    "print('Number of imported rows: ',imported_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = longitude\n",
    "y = latitude\n",
    "\n",
    "**location (lat,long)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long=df.loc[:,'x']\n",
    "lat=df.loc[:,'y']\n",
    "print(long)\n",
    "print(lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['39.63052999960336,-122.12948999962096',\n",
       " '39.68374999994705,-122.3026999996182',\n",
       " '39.254040000099046,-122.6757200000161',\n",
       " '39.99832999984903,-118.55497999993952',\n",
       " '39.2461200001521,-122.68026999926954',\n",
       " '39.2445200000478,-122.65733000033671',\n",
       " '40.874949999927,-120.99685000008893',\n",
       " '38.55013999946771,-122.56005000001664',\n",
       " '36.74385999949901,-120.01720999973608',\n",
       " '39.64973999977349,-121.3529600003908']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_lst=[]\n",
    "for i,j in enumerate(lat):\n",
    "    lat_long=str(j)+','+str(long[i])\n",
    "    locations_lst.append(lat_long)\n",
    "locations_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prompt data from Weather API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "import csv\n",
    "import codecs\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import sys\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 4, 22)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def API_query_forecast(loc, n, apikey='Z3Z4Q9PHE3MSXWKUPQNQ8434D', unit='metric', agg='24'):\n",
    "    # This is the core of our weather query URL\n",
    "    BaseURL = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/forecast'\n",
    "\n",
    "    ApiKey=apikey\n",
    "    #UnitGroup sets the units of the output - us or metric\n",
    "    UnitGroup=unit\n",
    "\n",
    "    #Locations for the weather data. Multiple locations separated by pipe (|)\n",
    "    Locations=loc\n",
    "\n",
    "    #1=hourly, 24=daily\n",
    "    AggregateHours=agg\n",
    "\n",
    "    #Forecasting days\n",
    "    forecastDays=n\n",
    "\n",
    "    # Set up the query\n",
    "    QueryParams = '?aggregateHours=' + AggregateHours + '&forecastDays=' +  forecastDays + '&unitGroup=' + UnitGroup + '&shortColumnNames=true'\n",
    "\n",
    "    Locations='&locations=' + Locations + '&locationMode=array'\n",
    "\n",
    "    ApiKey='&key='+ApiKey\n",
    "\n",
    "    # Build the entire query\n",
    "    URL = BaseURL + QueryParams + Locations + ApiKey+\"&contentType=json\"\n",
    "\n",
    "    return URL\n",
    "\n",
    "#building header, only need to loop once\n",
    "def json_header_extractor():\n",
    "    dict_locations=weatherData['locations'][0]\n",
    "    header=[]\n",
    "    for key in dict_locations.keys():\n",
    "        if key=='values':\n",
    "            header.extend(list(dict_locations[key][0].keys()))\n",
    "        elif key=='currentConditions':\n",
    "            header.extend(list(dict_locations[key].keys()))\n",
    "        else:\n",
    "            header.append(key)\n",
    "    return header\n",
    "\n",
    "#building data, in the 'values' section, there is the data for the forecasted days, so if forecstDays=3, there is 3 elements in the list 'values'\n",
    "def json_data_extractor():\n",
    "    dict_locations=weatherData['locations'][0]\n",
    "    all_data=[]\n",
    "    for i in range(forecastDays):\n",
    "        data=[]\n",
    "        for key, value in dict_locations.items():\n",
    "            if key=='values':\n",
    "                data.extend(list(dict_locations[key][i].values()))\n",
    "            elif key=='currentConditions':\n",
    "                data.extend(list(dict_locations[key].values()))\n",
    "            else:\n",
    "                data.append(value)\n",
    "        all_data.append(data)\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather API from Visual Crossing, is supposed to support more than 1 location (mention in the documentation: https://www.visualcrossing.com/resources/documentation/weather-api/weather-api-json-result-structure/). It says \"Data for multiple locations can be requested in a single request by concatenating multiple locations using the pipe (|) character.\". I tried everything, even contacting the API provider, it remains unsuccesful. \n",
    "Therefore, I opt for a 'for loop' over a list of locations and change the content type for '.csv' over '.json' so I can manipulate the data easliy. \n",
    "Unfortunately, there is not the same amout of variables in .csv file, therefore we will go through .json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt data from API and extract JSON data to a DataFrame:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/forecast?aggregateHours=24&forecastDays=3&unitGroup=metric&shortColumnNames=true&locations=LosAngeles,CA&locationMode=array&key=Z3Z4Q9PHE3MSXWKUPQNQ8434D&contentType=json\n",
      "https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/forecast?aggregateHours=24&forecastDays=3&unitGroup=metric&shortColumnNames=true&locations=Bakersfield,CA&locationMode=array&key=Z3Z4Q9PHE3MSXWKUPQNQ8434D&contentType=json\n",
      "https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/forecast?aggregateHours=24&forecastDays=3&unitGroup=metric&shortColumnNames=true&locations=SanFrancisco,CA&locationMode=array&key=Z3Z4Q9PHE3MSXWKUPQNQ8434D&contentType=json\n"
     ]
    }
   ],
   "source": [
    "locations_lst=['LosAngeles,CA','Bakersfield,CA','SanFrancisco,CA']\n",
    "forecastDays=3\n",
    "n=0\n",
    "raw_data=[]\n",
    "for loc in locations_lst:           #location_lst --> locations from S3 buckets\n",
    "    URL=API_query_forecast(loc, str(forecastDays))\n",
    "    print(URL)\n",
    "    response = urllib.request.urlopen(URL)\n",
    "    data = response.read()\n",
    "    weatherData = json.loads(data.decode('utf-8'))\n",
    "    if n==0: \n",
    "        header=json_header_extractor()\n",
    "        data=json_data_extractor()\n",
    "        raw_data.extend(data)\n",
    "        n+=1\n",
    "    else:\n",
    "        data=json_data_extractor()\n",
    "        raw_data.extend(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build DataFrame and procceed to data manipulations to load in RDS environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build DataFrame\n",
    "df=pd.DataFrame(raw_data,columns=header)\n",
    "df_manip=df\n",
    "\n",
    "#Quick manipulation in order to facilitate the insertion  in the RDS tables (delete columns with no data, and re-arrange column order)\n",
    "# delete 'stationContribution', 'info', 'index', 'distance', 'time', 'currentConditions' , 'alerts' because they are empty columns -> no data\n",
    "df_manip=df_manip.drop(['stationContributions', 'datetime', 'index', 'distance', 'time','stations', 'alerts'], axis=1)\n",
    "\n",
    "# re-arrange column\n",
    "re_arrg_col=['tz','longitude','latitude','name','address','id']\n",
    "for col in re_arrg_col:\n",
    "    shiftpos=df_manip.pop(col)\n",
    "    df_manip.insert(0,col,shiftpos)\n",
    "\n",
    "#rename some columns (ex: 2 times wdir --> wdir & wdir_current)\n",
    "#the last part of the table are current conditons (starts after 'cape' column)\n",
    "index_current_cond=df_manip.columns.get_loc('cape')\n",
    "lst_to_rename=list(df_manip.columns[index_current_cond:])\n",
    "for i,col in enumerate(lst_to_rename):\n",
    "    if col in ['cape','sunrise','moonphase','sunset']:\n",
    "        pass\n",
    "    else:\n",
    "        new_col=col+'_current'\n",
    "        lst_to_rename[i]=new_col\n",
    "lst_not_modify=list(df_manip.columns[:index_current_cond])\n",
    "new_columns=lst_not_modify+lst_to_rename\n",
    "df_manip.columns=new_columns\n",
    "\n",
    "#Save the DataFrame to .csv\n",
    "new_df=df_manip\n",
    "new_df.to_csv('weather_API_realtime_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load data in RDS data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = os.environ['ENDPOINT']\n",
    "DB_NAME = os.environ['DB_NAME']\n",
    "USERNAME = os.environ['USERNAME']\n",
    "PASSWORD = os.environ['PASSWORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    #print(\"host={} dbname={} user={} password={}\".format(ENDPOINT, DB_NAME, USERNAME, PASSWORD))\n",
    "    #conn = psycopg2.connect(\"host={} dbname={} user={} password={}\".format(ENDPOINT, DB_NAME, USERNAME, PASSWORD))\n",
    "    conn = psycopg2.connect(\"host=datalake.cbqjcib7k7og.us-east-1.rds.amazonaws.com dbname=datalake_weather user=nathan password=123456789\")\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not make connection to the Postgres database\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    cur = conn.cursor()\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not get curser to the Database\")\n",
    "    print(e)\n",
    "    \n",
    "# Auto commit is very important\n",
    "conn.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"CREATE TABLE IF NOT EXISTS REALTIME_WEATHER_DATA (id VARCHAR(70), address VARCHAR(80), name VARCHAR(70), latitude FLOAT, longitude FLOAT, tz VARCHAR(30), wdir FLOAT,uvindex FLOAT, datetimeStr DATE, preciptype VARCHAR(50), cin FLOAT, cloudcover FLOAT, pop FLOAT, mint FLOAT, precip FLOAT, solarradiation FLOAT, dew FLOAT, humidity FLOAT, temp FLOAT, maxt FLOAT, visibility FLOAT, wspd FLOAT,severerisk INT, solarenergy FLOAT, heatindex FLOAT, snowdepth FLOAT, sealevelpressure FLOAT, snow FLOAT, wgust FLOAT, conditions VARCHAR(90), windchill FLOAT, cape FLOAT, wdir_current FLOAT, temp_current FLOAT, sunrise TIMESTAMP, visibility_current INT, wspd_current FLOAT, icon_current VARCHAR(20), heatindex_current FLOAT, cloudcover_current FLOAT, precip_current FLOAT, moonphase FLOAT, snowdepth_current FLOAT, sealevelpressure_current FLOAT, dew_current FLOAT, sunset TIMESTAMP, humidity_current FLOAT, wgust_current FLOAT, windchill_current FLOAT);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "server closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\nserver closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\natr\\Desktop\\HSLU_S3\\DWL\\wild_fires_project\\DataLake_Wildfires_California\\realtime_weather_lambda.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/natr/Desktop/HSLU_S3/DWL/wild_fires_project/DataLake_Wildfires_California/realtime_weather_lambda.ipynb#ch0000030?line=0'>1</a>\u001b[0m cur\u001b[39m.\u001b[39;49mexecute(\u001b[39m'\u001b[39;49m\u001b[39mTRUNCATE TABLE REALTIME_WEATHER_DATA\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;31mOperationalError\u001b[0m: server closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\nserver closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\n"
     ]
    }
   ],
   "source": [
    "cur.execute('TRUNCATE TABLE REALTIME_WEATHER_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of imported rows:  9\n"
     ]
    }
   ],
   "source": [
    "#Upload data in the table\n",
    "data=new_df\n",
    "\n",
    "cols = \",\".join([str(i) for i in data.columns.tolist()])\n",
    "\n",
    "imported_rows=0\n",
    "excluded_rows_index=[]\n",
    "for i,row in data.iterrows():\n",
    "    try:\n",
    "        sql = \"INSERT INTO REALTIME_WEATHER_DATA  (\" +cols + \") VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "        cur.execute(sql, tuple(row))\n",
    "        conn.commit()\n",
    "        imported_rows+=1\n",
    "    except:\n",
    "        excluded_rows_index.append(i)\n",
    "        print(i)\n",
    "print('Number of imported rows: ',imported_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"select count(*) from REALTIME_WEATHER_DATA;\")\n",
    "number_rows=cur.fetchall()[0][0]\n",
    "print(number_rows)\n",
    "print(number_rows==imported_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda_realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "import pandas\n",
    "import boto3\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    # 1. Prompt data from S3 bucket-->ThermoAnomalies\n",
    "    s3_client =boto3.client('s3')\n",
    "    s3_bucket_name='eswf1'\n",
    "    s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id='ASIAYQYC6XJOIJZCJMA6',\n",
    "                    aws_secret_access_key='qECOWRW2FJq1PkJYZRELRPPCUtqjwidcbxMfjZVG',\n",
    "                    aws_session_token='FwoGZXIvYXdzEKP//////////wEaDPK9bNSky8503D+LWiK5AWPS6oas8dggTjCxMdV2BkWEto8DZUnJMfhIYZNJpGPSXaEUVssg4r6tkqD+EwYwBY5+PpZeifnvUMhEzTxI/f5xEl06TerGWyy/diEIsigJ9NjaqwoQb+5gWb36z6e348RraRBgmU3OWMDDXdjo/IJgA0tllDO/ZdjCJkyRYvfwaRC+NJAuHN79NgbhdmqWmD7rHsoH0RCtKJF3MVy8aagy1cW2iqf/3+9e+G8ltVh6H4wUQJO0ZrjbKNWbm5IGMi37TlL+egRjQ+kd85E4ENzweKRo19K+g5Smrxml/g8D3CiW6NkMzQ+tmHbrFaA=')\n",
    "    \n",
    "    my_bucket=s3.Bucket(s3_bucket_name)\n",
    "    \n",
    "    #file of interest 'ThAnomalies3.csv'\n",
    "    obj = s3.Object(s3_bucket_name,key='ThAnomalies3.csv')\n",
    "    data=obj.get()['Body'].read()\n",
    "    df=pd.read_csv(io.BytesIO(data), header=0, delimiter=\",\", low_memory=False)\n",
    "    \n",
    "    print('Data from S3 successfully accessed!')\n",
    "    \n",
    "    #2. Load ThermoAnomalies data in RDS \n",
    "    try: \n",
    "        #print(\"host={} dbname={} user={} password={}\".format(ENDPOINT, DB_NAME, USERNAME, PASSWORD))\n",
    "        #conn = psycopg2.connect(\"host={} dbname={} user={} password={}\".format(ENDPOINT, DB_NAME, USERNAME, PASSWORD))\n",
    "        conn = psycopg2.connect(\"host=datalake.cbqjcib7k7og.us-east-1.rds.amazonaws.com dbname=datalake_weather user=nathan password=123456789\")\n",
    "    except psycopg2.Error as e: \n",
    "        print(\"Error: Could not make connection to the Postgres database\")\n",
    "        print(e) \n",
    "    try: \n",
    "        cur = conn.cursor()\n",
    "    except psycopg2.Error as e: \n",
    "        print(\"Error: Could not get curser to the Database\")\n",
    "        print(e)\n",
    "        \n",
    "    conn.set_session(autocommit=True)\n",
    "    \n",
    "    cur.execute(\"CREATE TABLE IF NOT EXISTS THERMOANOMALIES_DATA (OBJECTID INT, BRIGHTNESS FLOAT, SCAN FLOAT, TRACK FLOAT, SATELLITE VARCHAR(10), CONFIDENCE FLOAT, VERSION VARCHAR(15), BRIGHT_T31 FLOAT, FRP FLOAT, ACQ_DATE FLOAT, DAYNIGHT VARCHAR(10), x FLOAT, y FLOAT)\")  \n",
    "    \n",
    "    #Upload data in the table\n",
    "    data=df\n",
    "\n",
    "    cols = \",\".join([str(i) for i in data.columns.tolist()])\n",
    "\n",
    "    for i,row in data.iterrows():\n",
    "        try:\n",
    "            sql = \"INSERT INTO THERMOANOMALIES_DATA  (\" +cols + \") VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "            cur.execute(sql, tuple(row))\n",
    "            conn.commit()\n",
    "        except:\n",
    "         pass\n",
    "    \n",
    "    print('ThAnomalies data successfully loaded to RDS!')\n",
    "    #Access latitude and longitute\n",
    "    long=df.loc[:,'x']\n",
    "    lat=df.loc[:,'y']\n",
    "    locations_lst=[]\n",
    "    for i,j in enumerate(lat):\n",
    "        lat_long=str(j)+','+str(long[i])\n",
    "        locations_lst.append(lat_long)\n",
    "    \n",
    "    #3. Prompt data from the weather API\n",
    "    \n",
    "    #Query builder function\n",
    "    def API_query_forecast(loc, n, apikey='Z3Z4Q9PHE3MSXWKUPQNQ8434D', unit='metric', agg='24'):\n",
    "        # This is the core of our weather query URL\n",
    "        BaseURL = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/forecast'\n",
    "    \n",
    "        ApiKey=apikey\n",
    "        #UnitGroup sets the units of the output - us or metric\n",
    "        UnitGroup=unit\n",
    "    \n",
    "        #Locations for the weather data. Multiple locations separated by pipe (|)\n",
    "        Locations=loc\n",
    "    \n",
    "        #1=hourly, 24=daily\n",
    "        AggregateHours=agg\n",
    "    \n",
    "        #Forecasting days\n",
    "        forecastDays=n\n",
    "    \n",
    "        # Set up the query\n",
    "        QueryParams = '?aggregateHours=' + AggregateHours + '&forecastDays=' +  forecastDays + '&unitGroup=' + UnitGroup + '&shortColumnNames=true'\n",
    "    \n",
    "        Locations='&locations=' + Locations + '&locationMode=array'\n",
    "    \n",
    "        ApiKey='&key='+ApiKey\n",
    "    \n",
    "        # Build the entire query\n",
    "        URL = BaseURL + QueryParams + Locations + ApiKey+\"&contentType=json\"\n",
    "    \n",
    "        return URL\n",
    "        \n",
    "    #Header JSON extractor function\n",
    "    def json_header_extractor():\n",
    "        dict_locations=weatherData['locations'][0]\n",
    "        header=[]\n",
    "        for key in dict_locations.keys():\n",
    "            if key=='values':\n",
    "                header.extend(list(dict_locations[key][0].keys()))\n",
    "            elif key=='currentConditions':\n",
    "                header.extend(list(dict_locations[key].keys()))\n",
    "            else:\n",
    "                header.append(key)\n",
    "        return header\n",
    "        \n",
    "    #Data JSON extractor function  \n",
    "    def json_data_extractor():\n",
    "        dict_locations=weatherData['locations'][0]\n",
    "        all_data=[]\n",
    "        for i in range(forecastDays):\n",
    "            data=[]\n",
    "            for key, value in dict_locations.items():\n",
    "                if key=='values':\n",
    "                    data.extend(list(dict_locations[key][i].values()))\n",
    "                elif key=='currentConditions':\n",
    "                    data.extend(list(dict_locations[key].values()))\n",
    "                else:\n",
    "                    data.append(value)\n",
    "            all_data.append(data)\n",
    "        return all_data\n",
    "    \n",
    "    #Access the API and extract the data\n",
    "    forecastDays=3\n",
    "    n=0\n",
    "    raw_data=[]\n",
    "    for loc in locations_lst:           #location_lst --> locations from S3 buckets\n",
    "        URL=API_query_forecast(loc, str(forecastDays))\n",
    "        print(URL)\n",
    "        response = urllib.request.urlopen(URL)\n",
    "        data = response.read()\n",
    "        weatherData = json.loads(data.decode('utf-8'))\n",
    "        if n==0: \n",
    "            header=json_header_extractor()\n",
    "            data=json_data_extractor()\n",
    "            raw_data.extend(data)\n",
    "            n+=1\n",
    "        else:\n",
    "            data=json_data_extractor()\n",
    "            raw_data.extend(data)\n",
    "    \n",
    "    #Build DateFrame and procceed to minor manipulations to load the data in RDS\n",
    "    df=pd.DataFrame(raw_data,columns=header)\n",
    "    df_manip=df\n",
    "    \n",
    "    #Quick manipulation in order to facilitate the insertion  in the RDS tables (delete columns with no data, and re-arrange column order)\n",
    "    # delete 'stationContribution', 'info', 'index', 'distance', 'time', 'currentConditions' , 'alerts' because they are empty columns -> no data\n",
    "    df_manip=df_manip.drop(['stationContributions', 'datetime', 'index', 'distance', 'time','stations', 'alerts'], axis=1)\n",
    "    \n",
    "    # re-arrange column\n",
    "    re_arrg_col=['tz','longitude','latitude','name','address','id']\n",
    "    for col in re_arrg_col:\n",
    "        shiftpos=df_manip.pop(col)\n",
    "        df_manip.insert(0,col,shiftpos)\n",
    "    \n",
    "    # rename some columns (ex: 2 times wdir --> wdir & wdir_current)\n",
    "    # the last part of the table are current conditons (starts after 'cape' column)\n",
    "    index_current_cond=df_manip.columns.get_loc('cape')\n",
    "    lst_to_rename=list(df_manip.columns[index_current_cond:])\n",
    "    for i,col in enumerate(lst_to_rename):\n",
    "        if col in ['cape','sunrise','moonphase','sunset']:\n",
    "            pass\n",
    "        else:\n",
    "            new_col=col+'_current'\n",
    "            lst_to_rename[i]=new_col\n",
    "    lst_not_modify=list(df_manip.columns[:index_current_cond])\n",
    "    new_columns=lst_not_modify+lst_to_rename\n",
    "    df_manip.columns=new_columns\n",
    "    \n",
    "    new_data=df_manip\n",
    "    \n",
    "    print('Data from API successfully accessed and DataFrame built!')\n",
    "    \n",
    "    #4. Load weather data in RDS\n",
    "    try: \n",
    "        conn = psycopg2.connect(\"host=datalake.cbqjcib7k7og.us-east-1.rds.amazonaws.com dbname=datalake_weather user=nathan password=123456789\")\n",
    "    except psycopg2.Error as e: \n",
    "        print(\"Error: Could not make connection to the Postgres database\")\n",
    "        print(e)\n",
    "    try: \n",
    "        cur = conn.cursor()\n",
    "    except psycopg2.Error as e: \n",
    "        print(\"Error: Could not get curser to the Database\")\n",
    "        print(e)\n",
    "\n",
    "    conn.set_session(autocommit=True)\n",
    "    \n",
    "    cur.execute(\"CREATE TABLE IF NOT EXISTS REALTIME_WEATHER_DATA (id VARCHAR(70), address VARCHAR(80), name VARCHAR(70), latitude FLOAT, longitude FLOAT, tz VARCHAR(30), wdir FLOAT,uvindex FLOAT, datetimeStr DATE, preciptype VARCHAR(50), cin FLOAT, cloudcover FLOAT, pop FLOAT, mint FLOAT, precip FLOAT, solarradiation FLOAT, dew FLOAT, humidity FLOAT, temp FLOAT, maxt FLOAT, visibility FLOAT, wspd FLOAT,severerisk INT, solarenergy FLOAT, heatindex FLOAT, snowdepth FLOAT, sealevelpressure FLOAT, snow FLOAT, wgust FLOAT, conditions VARCHAR(90), windchill FLOAT, cape FLOAT, wdir_current FLOAT, temp_current FLOAT, sunrise TIMESTAMP, visibility_current INT, wspd_current FLOAT, icon_current VARCHAR(20), heatindex_current FLOAT, cloudcover_current FLOAT, precip_current FLOAT, moonphase FLOAT, snowdepth_current FLOAT, sealevelpressure_current FLOAT, dew_current FLOAT, sunset TIMESTAMP, humidity_current FLOAT, wgust_current FLOAT, windchill_current FLOAT);\")\n",
    "\n",
    "    #Upload data in the table\n",
    "    data=new_df\n",
    "    \n",
    "    cols = \",\".join([str(i) for i in data.columns.tolist()])\n",
    "    imported_rows=0\n",
    "    for i,row in data.iterrows():\n",
    "        try:\n",
    "            sql = \"INSERT INTO REALTIME_WEATHER_DATA  (\" +cols + \") VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "            cur.execute(sql, tuple(row))\n",
    "            conn.commit()\n",
    "            imported_rows+=1\n",
    "        except:\n",
    "            pass\n",
    "    #Run a little check\n",
    "    cur.execute(\"select count(*) from REALTIME_WEATHER_DATA;\")\n",
    "    number_rows=cur.fetchall()[0][0]\n",
    "    print(number_rows)\n",
    "    print(number_rows==imported_rows)\n",
    "    \n",
    "    # END\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return 'ThermoAnomalies accessed and loaded & Weather Data accessed and loaded in RDS'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6b7c4741759e6dff2d3609019adb5b2eb8766476d7e61b1a583782d03f4e08b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
